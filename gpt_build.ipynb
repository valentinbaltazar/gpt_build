{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\n\n- Building a GPT from scratch, following Andrej Karpathy's lesson at: https://youtu.be/kCc8FmEb1nY?si=4AZNnwmnb4XflQT8\n- Character level model, trained on Shakespears works","metadata":{}},{"cell_type":"code","source":"# clone repo for input text files\n!git clone https://github.com/karpathy/ng-video-lecture.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:28:55.846303Z","iopub.execute_input":"2025-03-27T05:28:55.846562Z","iopub.status.idle":"2025-03-27T05:28:56.597340Z","shell.execute_reply.started":"2025-03-27T05:28:55.846531Z","shell.execute_reply":"2025-03-27T05:28:56.595942Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'ng-video-lecture'...\nremote: Enumerating objects: 64, done.\u001b[K\nremote: Counting objects: 100% (32/32), done.\u001b[K\nremote: Compressing objects: 100% (14/14), done.\u001b[K\nremote: Total 64 (delta 22), reused 18 (delta 18), pack-reused 32 (from 1)\u001b[K\nReceiving objects: 100% (64/64), 441.23 KiB | 9.00 MiB/s, done.\nResolving deltas: 100% (23/23), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# open file we will use to train on\nfile_path = '/kaggle/working/ng-video-lecture/input.txt'\nwith open(file_path, 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:28:56.599442Z","iopub.execute_input":"2025-03-27T05:28:56.599927Z","iopub.status.idle":"2025-03-27T05:28:56.607771Z","shell.execute_reply.started":"2025-03-27T05:28:56.599882Z","shell.execute_reply":"2025-03-27T05:28:56.606179Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print(\"Total chracters in data set:\", len(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:28:56.610420Z","iopub.execute_input":"2025-03-27T05:28:56.610766Z","iopub.status.idle":"2025-03-27T05:28:56.629763Z","shell.execute_reply.started":"2025-03-27T05:28:56.610708Z","shell.execute_reply":"2025-03-27T05:28:56.628623Z"}},"outputs":[{"name":"stdout","text":"Total chracters in data set: 1115394\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# some samples from the training set\nprint(text[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:28:56.632078Z","iopub.execute_input":"2025-03-27T05:28:56.632529Z","iopub.status.idle":"2025-03-27T05:28:56.652872Z","shell.execute_reply.started":"2025-03-27T05:28:56.632486Z","shell.execute_reply":"2025-03-27T05:28:56.651602Z"}},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# all unique characters\nchars = set(text)\nvocab_size = len(chars)\nprint(f\"{vocab_size} char set:\", ''.join(sorted(list(chars))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:28:56.653990Z","iopub.execute_input":"2025-03-27T05:28:56.654356Z","iopub.status.idle":"2025-03-27T05:28:56.692135Z","shell.execute_reply.started":"2025-03-27T05:28:56.654316Z","shell.execute_reply":"2025-03-27T05:28:56.690888Z"}},"outputs":[{"name":"stdout","text":"65 char set: \n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"sorted_chars = sorted(list(chars))\nfor i in range(vocab_size):\n    # checking space & new line characters\n    c = sorted_chars[i]\n    print(f\"{i}: {ord(c)} '{c}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:28:56.693317Z","iopub.execute_input":"2025-03-27T05:28:56.693753Z","iopub.status.idle":"2025-03-27T05:28:56.725797Z","shell.execute_reply.started":"2025-03-27T05:28:56.693692Z","shell.execute_reply":"2025-03-27T05:28:56.724202Z"}},"outputs":[{"name":"stdout","text":"0: 10 '\n'\n1: 32 ' '\n2: 33 '!'\n3: 36 '$'\n4: 38 '&'\n5: 39 '''\n6: 44 ','\n7: 45 '-'\n8: 46 '.'\n9: 51 '3'\n10: 58 ':'\n11: 59 ';'\n12: 63 '?'\n13: 65 'A'\n14: 66 'B'\n15: 67 'C'\n16: 68 'D'\n17: 69 'E'\n18: 70 'F'\n19: 71 'G'\n20: 72 'H'\n21: 73 'I'\n22: 74 'J'\n23: 75 'K'\n24: 76 'L'\n25: 77 'M'\n26: 78 'N'\n27: 79 'O'\n28: 80 'P'\n29: 81 'Q'\n30: 82 'R'\n31: 83 'S'\n32: 84 'T'\n33: 85 'U'\n34: 86 'V'\n35: 87 'W'\n36: 88 'X'\n37: 89 'Y'\n38: 90 'Z'\n39: 97 'a'\n40: 98 'b'\n41: 99 'c'\n42: 100 'd'\n43: 101 'e'\n44: 102 'f'\n45: 103 'g'\n46: 104 'h'\n47: 105 'i'\n48: 106 'j'\n49: 107 'k'\n50: 108 'l'\n51: 109 'm'\n52: 110 'n'\n53: 111 'o'\n54: 112 'p'\n55: 113 'q'\n56: 114 'r'\n57: 115 's'\n58: 116 't'\n59: 117 'u'\n60: 118 'v'\n61: 119 'w'\n62: 120 'x'\n63: 121 'y'\n64: 122 'z'\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Encoder/Decoder: each character maps to an integer which its the indx in the sorted array\nc_to_i = {c:i for i,c in enumerate(sorted_chars)}\ni_to_c = {i:c for i,c in enumerate(sorted_chars)}\n\ndef encode(s):\n    # takes input string to encode\n    return [c_to_i[c] for c in s]\n\n\ndef decode(l):\n    # takes an array of integers to decode\n    return [i_to_c[i] for i in l]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:28:56.729700Z","iopub.execute_input":"2025-03-27T05:28:56.730209Z","iopub.status.idle":"2025-03-27T05:28:56.744083Z","shell.execute_reply.started":"2025-03-27T05:28:56.730166Z","shell.execute_reply":"2025-03-27T05:28:56.743009Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(encode(\"Hello World this is GPT\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:28:56.745278Z","iopub.execute_input":"2025-03-27T05:28:56.745692Z","iopub.status.idle":"2025-03-27T05:28:56.767806Z","shell.execute_reply.started":"2025-03-27T05:28:56.745651Z","shell.execute_reply":"2025-03-27T05:28:56.766585Z"}},"outputs":[{"name":"stdout","text":"[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 1, 58, 46, 47, 57, 1, 47, 57, 1, 19, 28, 32]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"x = encode(\"Hello World this is GPT\")\nprint(decode(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:28:56.770931Z","iopub.execute_input":"2025-03-27T05:28:56.771237Z","iopub.status.idle":"2025-03-27T05:28:56.789034Z","shell.execute_reply.started":"2025-03-27T05:28:56.771211Z","shell.execute_reply":"2025-03-27T05:28:56.787710Z"}},"outputs":[{"name":"stdout","text":"['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'G', 'P', 'T']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"- Many other encoding schemas are possible, chat GPT and others mainly use **sub-word** tokenizers\n- Here this simple model has a tradeoff that a small vocab size for the encoder == large vector encodings\n- if we had a much larger vocab size == smaller vector encoding","metadata":{}},{"cell_type":"markdown","source":"# Pytorch\n- We will use pytorch here to generate the data as a tensor for training","metadata":{}},{"cell_type":"code","source":"import torch\ndata = torch.tensor(encode(text)) # Tensor (m,) == 1D vector --> 1D array\nprint(data.shape, data.dtype)\nprint(data[:1000]) # 1000 characters encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:28:56.790549Z","iopub.execute_input":"2025-03-27T05:28:56.790947Z","iopub.status.idle":"2025-03-27T05:29:01.372906Z","shell.execute_reply.started":"2025-03-27T05:28:56.790884Z","shell.execute_reply":"2025-03-27T05:29:01.371686Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# training and validation splits\nn = int(0.9*len(data))\ntrain = data[:n]\nval = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:29:01.373975Z","iopub.execute_input":"2025-03-27T05:29:01.374533Z","iopub.status.idle":"2025-03-27T05:29:01.379594Z","shell.execute_reply.started":"2025-03-27T05:29:01.374481Z","shell.execute_reply":"2025-03-27T05:29:01.378580Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# we will train on chunks of the data\nblock_size = 8 # this will be the max context X to predict target Y\ntrain[:block_size + 1] # we can train on 8 subarrays/instances and predict the next 1 given 9 total seq len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:29:01.380667Z","iopub.execute_input":"2025-03-27T05:29:01.381045Z","iopub.status.idle":"2025-03-27T05:29:01.414688Z","shell.execute_reply.started":"2025-03-27T05:29:01.381009Z","shell.execute_reply":"2025-03-27T05:29:01.413445Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# example of a training instance\nx = train[:block_size]\ny = train[1:block_size + 1]\n\nfor i in range(block_size):\n    context = x[:i+1] # given the context predict y\n    target = y[i] # only predicts the next chracter in a given seq\n    print(\"Context:\", context, \"Target:\", target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:29:01.415925Z","iopub.execute_input":"2025-03-27T05:29:01.416363Z","iopub.status.idle":"2025-03-27T05:29:01.449391Z","shell.execute_reply.started":"2025-03-27T05:29:01.416330Z","shell.execute_reply":"2025-03-27T05:29:01.448283Z"}},"outputs":[{"name":"stdout","text":"Context: tensor([18]) Target: tensor(47)\nContext: tensor([18, 47]) Target: tensor(56)\nContext: tensor([18, 47, 56]) Target: tensor(57)\nContext: tensor([18, 47, 56, 57]) Target: tensor(58)\nContext: tensor([18, 47, 56, 57, 58]) Target: tensor(1)\nContext: tensor([18, 47, 56, 57, 58,  1]) Target: tensor(15)\nContext: tensor([18, 47, 56, 57, 58,  1, 15]) Target: tensor(47)\nContext: tensor([18, 47, 56, 57, 58,  1, 15, 47]) Target: tensor(58)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# create batch processing\ntorch.manual_seed(1337)\nbatch_size = 4\nblock_size = 8\n\ndef get_batch(split):\n    data = train if split == 'train' else val\n    ix = torch.randint(len(data) - block_size, (batch_size,)) # outputs 1D vector of len == batch_size\n    x = torch.stack([data[i:i+block_size] for i in ix]) # stacks the 1D vectors --> (4,8) 4 rows x 8 cols\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # (4,8) we will make 8 predictions y on each X\n    return x, y\n\nxb, yb = get_batch('train')\nprint(f\"Inputs: {xb.shape} \\n{xb} \\nTargets: {yb.shape} \\n{yb}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:29:01.451124Z","iopub.execute_input":"2025-03-27T05:29:01.451449Z","iopub.status.idle":"2025-03-27T05:29:01.488236Z","shell.execute_reply.started":"2025-03-27T05:29:01.451414Z","shell.execute_reply":"2025-03-27T05:29:01.487053Z"}},"outputs":[{"name":"stdout","text":"Inputs: torch.Size([4, 8]) \ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]]) \nTargets: torch.Size([4, 8]) \ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Total training samples per batch == batch_size * block_size == 8*4 == 32\nfor b in range(batch_size):\n    for t in range(block_size):\n        context = xb[b, :t+1] # sliding window context\n        target = yb[b,t] # predict the next chracter c\n        print(f\"Context: {context} Target: {target}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T05:29:01.489444Z","iopub.execute_input":"2025-03-27T05:29:01.489850Z","iopub.status.idle":"2025-03-27T05:29:01.515482Z","shell.execute_reply.started":"2025-03-27T05:29:01.489809Z","shell.execute_reply":"2025-03-27T05:29:01.514328Z"}},"outputs":[{"name":"stdout","text":"Context: tensor([24]) Target: 43\nContext: tensor([24, 43]) Target: 58\nContext: tensor([24, 43, 58]) Target: 5\nContext: tensor([24, 43, 58,  5]) Target: 57\nContext: tensor([24, 43, 58,  5, 57]) Target: 1\nContext: tensor([24, 43, 58,  5, 57,  1]) Target: 46\nContext: tensor([24, 43, 58,  5, 57,  1, 46]) Target: 43\nContext: tensor([24, 43, 58,  5, 57,  1, 46, 43]) Target: 39\nContext: tensor([44]) Target: 53\nContext: tensor([44, 53]) Target: 56\nContext: tensor([44, 53, 56]) Target: 1\nContext: tensor([44, 53, 56,  1]) Target: 58\nContext: tensor([44, 53, 56,  1, 58]) Target: 46\nContext: tensor([44, 53, 56,  1, 58, 46]) Target: 39\nContext: tensor([44, 53, 56,  1, 58, 46, 39]) Target: 58\nContext: tensor([44, 53, 56,  1, 58, 46, 39, 58]) Target: 1\nContext: tensor([52]) Target: 58\nContext: tensor([52, 58]) Target: 1\nContext: tensor([52, 58,  1]) Target: 58\nContext: tensor([52, 58,  1, 58]) Target: 46\nContext: tensor([52, 58,  1, 58, 46]) Target: 39\nContext: tensor([52, 58,  1, 58, 46, 39]) Target: 58\nContext: tensor([52, 58,  1, 58, 46, 39, 58]) Target: 1\nContext: tensor([52, 58,  1, 58, 46, 39, 58,  1]) Target: 46\nContext: tensor([25]) Target: 17\nContext: tensor([25, 17]) Target: 27\nContext: tensor([25, 17, 27]) Target: 10\nContext: tensor([25, 17, 27, 10]) Target: 0\nContext: tensor([25, 17, 27, 10,  0]) Target: 21\nContext: tensor([25, 17, 27, 10,  0, 21]) Target: 1\nContext: tensor([25, 17, 27, 10,  0, 21,  1]) Target: 54\nContext: tensor([25, 17, 27, 10,  0, 21,  1, 54]) Target: 39\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Bigram Language Model\n- Simplest possible NN model for LLM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token reads off the logits for the next token from this lookup table of 65x65\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n    \n    def forward(self, idx, targets=None):\n        \n        logits = self.token_embedding_table(idx) # (B,T,Channels=vocab_size) == (4,8,65)\n\n        if targets is None:\n            loss = None\n        else:\n            # reshape input tensor to match pytorch docs F.cross_entropy\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C) # 2D tensor\n            targets = targets.view(B*T) # 1D Tensor\n    \n            # calculate the loss\n            loss = F.cross_entropy(logits, targets)\n            \n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # generate new token based on prev one\n        for _ in range(max_new_tokens):\n            # get opredictions\n            logits, loss = self(idx) # calling forward pass\n\n            # only look at last time step\n            logits = logits[:, -1, :] # --> (B,C) we only take the last T value for each row B in (B*T, C)\n\n            # apply softmax to get probabilities (partition function, probability distribution that maximizes entropy)\n            probs = F.softmax(logits, dim=-1) # (B,C)\n\n            # sample the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B,1) take one sample from the distribution\n\n            # append the sampled idx to the run for next sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n\n        return idx\n        \nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb) # __call__ is enabled for the \"forward\" method of classes\n\nprint(logits.shape)\nprint(loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:08:11.102647Z","iopub.execute_input":"2025-03-27T06:08:11.103061Z","iopub.status.idle":"2025-03-27T06:08:11.117461Z","shell.execute_reply.started":"2025-03-27T06:08:11.103030Z","shell.execute_reply":"2025-03-27T06:08:11.116412Z"}},"outputs":[{"name":"stdout","text":"torch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Now lets see how this compares to the theoretical loss which is realted to entropy == number of states\nimport numpy as np\n\nprint(-np.log(1/65))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:08:11.671821Z","iopub.execute_input":"2025-03-27T06:08:11.672192Z","iopub.status.idle":"2025-03-27T06:08:11.677755Z","shell.execute_reply.started":"2025-03-27T06:08:11.672161Z","shell.execute_reply":"2025-03-27T06:08:11.676380Z"}},"outputs":[{"name":"stdout","text":"4.174387269895637\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# initial predictions are not that great due to thier entropy\n# Here we will look at model generation\n\nidx = torch.zeros((1,1), dtype=torch.long) # 1x1 matrix of integer type, initialized to val = 0\n\nres = decode(m.generate(idx, max_new_tokens=100)[0].tolist())\n\nprint(''.join(res))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:10:33.635357Z","iopub.execute_input":"2025-03-27T06:10:33.635707Z","iopub.status.idle":"2025-03-27T06:10:33.658398Z","shell.execute_reply.started":"2025-03-27T06:10:33.635683Z","shell.execute_reply":"2025-03-27T06:10:33.657235Z"}},"outputs":[{"name":"stdout","text":"\nKuBZvrpxZQgC-hlkq,ptKqHoiX-jjeLJ &slERj KUsBOL!mpJO!zLg'wNfqHAMgq'hZCWhu.W.IBcP \nRFJ&DEs,nw?pxE?xjNH\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# First Generation & Training\n- This looks like garbage because we use a very simple bigram model where the next token predicted depends only on the current token, not the history\n- Next we will try and train it and see if it improves!","metadata":{}},{"cell_type":"code","source":"# lets try the same model on more data\nbatch_size = 32\n\n# pick an optimizer, SGD or ADAM\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nfor steps in range(10000):\n    # get sample batch data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True) # zero out grad from prev steps\n    loss.backward() # get gradients for all pramaters\n    optimizer.step() # update based on gradients\n\nprint(loss.item()) # look at loss improvement","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:21:06.966306Z","iopub.execute_input":"2025-03-27T06:21:06.966677Z","iopub.status.idle":"2025-03-27T06:21:26.925681Z","shell.execute_reply.started":"2025-03-27T06:21:06.966646Z","shell.execute_reply":"2025-03-27T06:21:26.924461Z"}},"outputs":[{"name":"stdout","text":"2.3964128494262695\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# lets look at model generation after 10k steps of learning\n\nidx = torch.zeros((1,1), dtype=torch.long) # 1x1 matrix of integer type, initialized to val = 0\n\nres = decode(m.generate(idx, max_new_tokens=100)[0].tolist())\n\nprint(''.join(res))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:22:12.993044Z","iopub.execute_input":"2025-03-27T06:22:12.993399Z","iopub.status.idle":"2025-03-27T06:22:13.016508Z","shell.execute_reply.started":"2025-03-27T06:22:12.993368Z","shell.execute_reply":"2025-03-27T06:22:13.015426Z"}},"outputs":[{"name":"stdout","text":"\nI MIO:\nGS:\nMI arsmer abepor f in g, hamen w\npo slirrcomyo putheuties;\n\nWA hest ns wis t:\nO,\n\nARGUESh\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# We have Shakespear! (From Temu)\n- This looks MUCH better after 10k steps of training!\n- Still not english words, but it has the style and formating of a Shakespear play","metadata":{}},{"cell_type":"code","source":"B,T,C = 4,8,2\n\nx = torch.randn(B,T,C)\nx.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:37:27.242226Z","iopub.execute_input":"2025-03-27T06:37:27.242614Z","iopub.status.idle":"2025-03-27T06:37:27.250739Z","shell.execute_reply.started":"2025-03-27T06:37:27.242585Z","shell.execute_reply":"2025-03-27T06:37:27.249495Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 2])"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"print(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:37:31.769683Z","iopub.execute_input":"2025-03-27T06:37:31.770074Z","iopub.status.idle":"2025-03-27T06:37:31.777844Z","shell.execute_reply.started":"2025-03-27T06:37:31.770043Z","shell.execute_reply":"2025-03-27T06:37:31.776672Z"}},"outputs":[{"name":"stdout","text":"tensor([[[ 1.2067,  0.8750],\n         [-0.7132, -0.1107],\n         [-0.6622, -1.9303],\n         [ 1.1462,  0.7672],\n         [ 0.5187, -2.4172],\n         [-1.3957, -1.5297],\n         [-0.4999,  0.8433],\n         [-1.1546, -2.3135]],\n\n        [[-2.2038,  0.2815],\n         [-0.1421, -0.3168],\n         [-0.7287,  1.1698],\n         [-0.6907,  0.0672],\n         [ 2.0157, -0.1807],\n         [-1.0268,  0.2443],\n         [ 1.1592, -0.9468],\n         [ 1.1652, -0.2175]],\n\n        [[-1.0334,  1.1528],\n         [ 0.9394, -0.1151],\n         [-0.4941, -1.6019],\n         [ 2.2417,  0.4767],\n         [-0.7743, -0.4969],\n         [ 1.9253, -0.6443],\n         [ 0.4159,  1.1258],\n         [-0.1532, -0.8104]],\n\n        [[-0.6214,  2.3315],\n         [-1.3047,  0.0709],\n         [ 1.3721,  1.0524],\n         [ 0.5432, -0.8030],\n         [-0.4247,  0.8274],\n         [-0.7501, -0.0136],\n         [ 0.7535,  0.5116],\n         [ 0.1882, -0.3833]]])\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"tx = x[0, :, :]\n\nprint(tx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T06:44:35.222741Z","iopub.execute_input":"2025-03-27T06:44:35.223115Z","iopub.status.idle":"2025-03-27T06:44:35.230365Z","shell.execute_reply.started":"2025-03-27T06:44:35.223085Z","shell.execute_reply":"2025-03-27T06:44:35.229390Z"}},"outputs":[{"name":"stdout","text":"tensor([[ 1.2067,  0.8750],\n        [-0.7132, -0.1107],\n        [-0.6622, -1.9303],\n        [ 1.1462,  0.7672],\n        [ 0.5187, -2.4172],\n        [-1.3957, -1.5297],\n        [-0.4999,  0.8433],\n        [-1.1546, -2.3135]])\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# cont at 1hr - 10 mins\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T07:01:19.323155Z","iopub.execute_input":"2025-03-27T07:01:19.323647Z","iopub.status.idle":"2025-03-27T07:01:19.327882Z","shell.execute_reply.started":"2025-03-27T07:01:19.323606Z","shell.execute_reply":"2025-03-27T07:01:19.326577Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}