{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\n\n- Building a GPT from scratch, following Andrej Karpathy's lesson at: https://youtu.be/kCc8FmEb1nY?si=4AZNnwmnb4XflQT8\n- Character level model, trained on Shakespears works","metadata":{}},{"cell_type":"code","source":"# clone repo for input text files\n!git clone https://github.com/karpathy/ng-video-lecture.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:28.380514Z","iopub.execute_input":"2025-04-03T05:30:28.380955Z","iopub.status.idle":"2025-04-03T05:30:29.310395Z","shell.execute_reply.started":"2025-04-03T05:30:28.380924Z","shell.execute_reply":"2025-04-03T05:30:29.308982Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'ng-video-lecture'...\nremote: Enumerating objects: 64, done.\u001b[K\nremote: Counting objects: 100% (32/32), done.\u001b[K\nremote: Compressing objects: 100% (14/14), done.\u001b[K\nremote: Total 64 (delta 22), reused 18 (delta 18), pack-reused 32 (from 1)\u001b[K\nReceiving objects: 100% (64/64), 441.23 KiB | 13.37 MiB/s, done.\nResolving deltas: 100% (23/23), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# open file we will use to train on\nfile_path = '/kaggle/working/ng-video-lecture/input.txt'\nwith open(file_path, 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:29.312272Z","iopub.execute_input":"2025-04-03T05:30:29.312716Z","iopub.status.idle":"2025-04-03T05:30:29.321493Z","shell.execute_reply.started":"2025-04-03T05:30:29.312666Z","shell.execute_reply":"2025-04-03T05:30:29.320209Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print(\"Total chracters in data set:\", len(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:29.323409Z","iopub.execute_input":"2025-04-03T05:30:29.323749Z","iopub.status.idle":"2025-04-03T05:30:29.332532Z","shell.execute_reply.started":"2025-04-03T05:30:29.323709Z","shell.execute_reply":"2025-04-03T05:30:29.331218Z"}},"outputs":[{"name":"stdout","text":"Total chracters in data set: 1115394\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# some samples from the training set\nprint(text[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:29.334454Z","iopub.execute_input":"2025-04-03T05:30:29.334792Z","iopub.status.idle":"2025-04-03T05:30:29.353241Z","shell.execute_reply.started":"2025-04-03T05:30:29.334768Z","shell.execute_reply":"2025-04-03T05:30:29.351980Z"}},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# all unique characters\nchars = set(text)\nvocab_size = len(chars)\nprint(f\"{vocab_size} char set:\", ''.join(sorted(list(chars))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:29.354522Z","iopub.execute_input":"2025-04-03T05:30:29.354892Z","iopub.status.idle":"2025-04-03T05:30:29.392495Z","shell.execute_reply.started":"2025-04-03T05:30:29.354861Z","shell.execute_reply":"2025-04-03T05:30:29.390958Z"}},"outputs":[{"name":"stdout","text":"65 char set: \n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"sorted_chars = sorted(list(chars))\nfor i in range(vocab_size):\n    # checking space & new line characters\n    c = sorted_chars[i]\n    print(f\"{i}: {ord(c)} '{c}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:29.393918Z","iopub.execute_input":"2025-04-03T05:30:29.394386Z","iopub.status.idle":"2025-04-03T05:30:29.422775Z","shell.execute_reply.started":"2025-04-03T05:30:29.394339Z","shell.execute_reply":"2025-04-03T05:30:29.421419Z"}},"outputs":[{"name":"stdout","text":"0: 10 '\n'\n1: 32 ' '\n2: 33 '!'\n3: 36 '$'\n4: 38 '&'\n5: 39 '''\n6: 44 ','\n7: 45 '-'\n8: 46 '.'\n9: 51 '3'\n10: 58 ':'\n11: 59 ';'\n12: 63 '?'\n13: 65 'A'\n14: 66 'B'\n15: 67 'C'\n16: 68 'D'\n17: 69 'E'\n18: 70 'F'\n19: 71 'G'\n20: 72 'H'\n21: 73 'I'\n22: 74 'J'\n23: 75 'K'\n24: 76 'L'\n25: 77 'M'\n26: 78 'N'\n27: 79 'O'\n28: 80 'P'\n29: 81 'Q'\n30: 82 'R'\n31: 83 'S'\n32: 84 'T'\n33: 85 'U'\n34: 86 'V'\n35: 87 'W'\n36: 88 'X'\n37: 89 'Y'\n38: 90 'Z'\n39: 97 'a'\n40: 98 'b'\n41: 99 'c'\n42: 100 'd'\n43: 101 'e'\n44: 102 'f'\n45: 103 'g'\n46: 104 'h'\n47: 105 'i'\n48: 106 'j'\n49: 107 'k'\n50: 108 'l'\n51: 109 'm'\n52: 110 'n'\n53: 111 'o'\n54: 112 'p'\n55: 113 'q'\n56: 114 'r'\n57: 115 's'\n58: 116 't'\n59: 117 'u'\n60: 118 'v'\n61: 119 'w'\n62: 120 'x'\n63: 121 'y'\n64: 122 'z'\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Encoder/Decoder: each character maps to an integer which its the indx in the sorted array\nc_to_i = {c:i for i,c in enumerate(sorted_chars)}\ni_to_c = {i:c for i,c in enumerate(sorted_chars)}\n\ndef encode(s):\n    # takes input string to encode\n    return [c_to_i[c] for c in s]\n\n\ndef decode(l):\n    # takes an array of integers to decode\n    return [i_to_c[i] for i in l]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:29.424272Z","iopub.execute_input":"2025-04-03T05:30:29.424574Z","iopub.status.idle":"2025-04-03T05:30:29.430318Z","shell.execute_reply.started":"2025-04-03T05:30:29.424549Z","shell.execute_reply":"2025-04-03T05:30:29.428987Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(encode(\"Hello World this is GPT\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:29.433862Z","iopub.execute_input":"2025-04-03T05:30:29.434204Z","iopub.status.idle":"2025-04-03T05:30:29.450421Z","shell.execute_reply.started":"2025-04-03T05:30:29.434178Z","shell.execute_reply":"2025-04-03T05:30:29.449142Z"}},"outputs":[{"name":"stdout","text":"[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 1, 58, 46, 47, 57, 1, 47, 57, 1, 19, 28, 32]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"x = encode(\"Hello World this is GPT\")\nprint(decode(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:29.453398Z","iopub.execute_input":"2025-04-03T05:30:29.453727Z","iopub.status.idle":"2025-04-03T05:30:29.468790Z","shell.execute_reply.started":"2025-04-03T05:30:29.453700Z","shell.execute_reply":"2025-04-03T05:30:29.467297Z"}},"outputs":[{"name":"stdout","text":"['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'G', 'P', 'T']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"- Many other encoding schemas are possible, chat GPT and others mainly use **sub-word** tokenizers\n- Here this simple model has a tradeoff that a small vocab size for the encoder == large vector encodings\n- if we had a much larger vocab size == smaller vector encoding","metadata":{}},{"cell_type":"markdown","source":"# Pytorch\n- We will use pytorch here to generate the data as a tensor for training","metadata":{}},{"cell_type":"code","source":"import torch\ndata = torch.tensor(encode(text)) # Tensor (m,) == 1D vector --> 1D array\nprint(data.shape, data.dtype)\nprint(data[:1000]) # 1000 characters encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:29.469878Z","iopub.execute_input":"2025-04-03T05:30:29.470266Z","iopub.status.idle":"2025-04-03T05:30:33.760141Z","shell.execute_reply.started":"2025-04-03T05:30:29.470231Z","shell.execute_reply":"2025-04-03T05:30:33.758817Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# training and validation splits\nn = int(0.9*len(data))\ntrain = data[:n]\nval = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:33.761532Z","iopub.execute_input":"2025-04-03T05:30:33.762141Z","iopub.status.idle":"2025-04-03T05:30:33.767072Z","shell.execute_reply.started":"2025-04-03T05:30:33.762099Z","shell.execute_reply":"2025-04-03T05:30:33.765771Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# we will train on chunks of the data\nblock_size = 8 # this will be the max context X to predict target Y\ntrain[:block_size + 1] # we can train on 8 subarrays/instances and predict the next 1 given 9 total seq len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:33.768428Z","iopub.execute_input":"2025-04-03T05:30:33.768791Z","iopub.status.idle":"2025-04-03T05:30:33.801219Z","shell.execute_reply.started":"2025-04-03T05:30:33.768756Z","shell.execute_reply":"2025-04-03T05:30:33.799952Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# example of a training instance\nx = train[:block_size]\ny = train[1:block_size + 1] # x --> offset by 1 \n\nfor i in range(block_size):\n    context = x[:i+1] # given the context predict y\n    target = y[i] # only predicts the next chracter in a given seq\n    print(\"Context:\", context, \"Target:\", target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:33.802618Z","iopub.execute_input":"2025-04-03T05:30:33.802944Z","iopub.status.idle":"2025-04-03T05:30:33.831443Z","shell.execute_reply.started":"2025-04-03T05:30:33.802907Z","shell.execute_reply":"2025-04-03T05:30:33.830357Z"}},"outputs":[{"name":"stdout","text":"Context: tensor([18]) Target: tensor(47)\nContext: tensor([18, 47]) Target: tensor(56)\nContext: tensor([18, 47, 56]) Target: tensor(57)\nContext: tensor([18, 47, 56, 57]) Target: tensor(58)\nContext: tensor([18, 47, 56, 57, 58]) Target: tensor(1)\nContext: tensor([18, 47, 56, 57, 58,  1]) Target: tensor(15)\nContext: tensor([18, 47, 56, 57, 58,  1, 15]) Target: tensor(47)\nContext: tensor([18, 47, 56, 57, 58,  1, 15, 47]) Target: tensor(58)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# create batch processing\ntorch.manual_seed(1337)\nbatch_size = 4\nblock_size = 8\n\ndef get_batch(split):\n    data = train if split == 'train' else val\n    ix = torch.randint(len(data) - block_size, (batch_size,)) # outputs 1D vector of len == batch_size\n    x = torch.stack([data[i:i+block_size] for i in ix]) # stacks the 1D vectors --> (4,8) 4 rows x 8 cols\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # (4,8) we will make 8 predictions y on each X\n    return x, y\n\nxb, yb = get_batch('train')\nprint(f\"Inputs: {xb.shape} \\n{xb} \\nTargets: {yb.shape} \\n{yb}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:33.832591Z","iopub.execute_input":"2025-04-03T05:30:33.832837Z","iopub.status.idle":"2025-04-03T05:30:33.867477Z","shell.execute_reply.started":"2025-04-03T05:30:33.832815Z","shell.execute_reply":"2025-04-03T05:30:33.866357Z"}},"outputs":[{"name":"stdout","text":"Inputs: torch.Size([4, 8]) \ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]]) \nTargets: torch.Size([4, 8]) \ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Total training samples per batch == batch_size * block_size == 8*4 == 32\nfor b in range(batch_size):\n    for t in range(block_size):\n        context = xb[b, :t+1] # sliding window context\n        target = yb[b,t] # predict the next chracter c\n        print(f\"Context: {context} Target: {target}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:33.868716Z","iopub.execute_input":"2025-04-03T05:30:33.868971Z","iopub.status.idle":"2025-04-03T05:30:33.895474Z","shell.execute_reply.started":"2025-04-03T05:30:33.868941Z","shell.execute_reply":"2025-04-03T05:30:33.894185Z"}},"outputs":[{"name":"stdout","text":"Context: tensor([24]) Target: 43\nContext: tensor([24, 43]) Target: 58\nContext: tensor([24, 43, 58]) Target: 5\nContext: tensor([24, 43, 58,  5]) Target: 57\nContext: tensor([24, 43, 58,  5, 57]) Target: 1\nContext: tensor([24, 43, 58,  5, 57,  1]) Target: 46\nContext: tensor([24, 43, 58,  5, 57,  1, 46]) Target: 43\nContext: tensor([24, 43, 58,  5, 57,  1, 46, 43]) Target: 39\nContext: tensor([44]) Target: 53\nContext: tensor([44, 53]) Target: 56\nContext: tensor([44, 53, 56]) Target: 1\nContext: tensor([44, 53, 56,  1]) Target: 58\nContext: tensor([44, 53, 56,  1, 58]) Target: 46\nContext: tensor([44, 53, 56,  1, 58, 46]) Target: 39\nContext: tensor([44, 53, 56,  1, 58, 46, 39]) Target: 58\nContext: tensor([44, 53, 56,  1, 58, 46, 39, 58]) Target: 1\nContext: tensor([52]) Target: 58\nContext: tensor([52, 58]) Target: 1\nContext: tensor([52, 58,  1]) Target: 58\nContext: tensor([52, 58,  1, 58]) Target: 46\nContext: tensor([52, 58,  1, 58, 46]) Target: 39\nContext: tensor([52, 58,  1, 58, 46, 39]) Target: 58\nContext: tensor([52, 58,  1, 58, 46, 39, 58]) Target: 1\nContext: tensor([52, 58,  1, 58, 46, 39, 58,  1]) Target: 46\nContext: tensor([25]) Target: 17\nContext: tensor([25, 17]) Target: 27\nContext: tensor([25, 17, 27]) Target: 10\nContext: tensor([25, 17, 27, 10]) Target: 0\nContext: tensor([25, 17, 27, 10,  0]) Target: 21\nContext: tensor([25, 17, 27, 10,  0, 21]) Target: 1\nContext: tensor([25, 17, 27, 10,  0, 21,  1]) Target: 54\nContext: tensor([25, 17, 27, 10,  0, 21,  1, 54]) Target: 39\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Bigram Language Model\n- Simplest possible NN model for LLM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token reads off the logits for the next token from this lookup table of 65x65\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n    \n    def forward(self, idx, targets=None):\n        \n        logits = self.token_embedding_table(idx) # (B,T,Channels=vocab_size) == (4,8,65)\n\n        if targets is None:\n            loss = None\n        else:\n            # reshape input tensor to match pytorch docs F.cross_entropy\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C) # 2D tensor\n            targets = targets.view(B*T) # 1D Tensor\n    \n            # calculate the loss\n            loss = F.cross_entropy(logits, targets)\n            \n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # generate new token based on prev one\n        for _ in range(max_new_tokens):\n            # get opredictions\n            logits, loss = self(idx) # calling forward pass\n\n            # only look at last time step\n            logits = logits[:, -1, :] # --> (B,C) we only take the last T value for each row B in (B*T, C)\n\n            # apply softmax to get probabilities (partition function, probability distribution that maximizes entropy)\n            probs = F.softmax(logits, dim=-1) # (B,C)\n\n            # sample the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B,1) take one sample from the distribution\n\n            # append the sampled idx to the run for next sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n\n        return idx\n        \nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb) # __call__ is enabled for the \"forward\" method of classes\n\nprint(logits.shape)\nprint(loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:33.896556Z","iopub.execute_input":"2025-04-03T05:30:33.896814Z","iopub.status.idle":"2025-04-03T05:30:33.958795Z","shell.execute_reply.started":"2025-04-03T05:30:33.896791Z","shell.execute_reply":"2025-04-03T05:30:33.957622Z"}},"outputs":[{"name":"stdout","text":"torch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Now lets see how this compares to the theoretical loss which is realted to entropy == number of states\nimport numpy as np\n\nprint(-np.log(1/65))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:33.959976Z","iopub.execute_input":"2025-04-03T05:30:33.960342Z","iopub.status.idle":"2025-04-03T05:30:33.966116Z","shell.execute_reply.started":"2025-04-03T05:30:33.960315Z","shell.execute_reply":"2025-04-03T05:30:33.964552Z"}},"outputs":[{"name":"stdout","text":"4.174387269895637\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# initial predictions are not that great due to thier entropy\n# Here we will look at model generation\n\nidx = torch.zeros((1,1), dtype=torch.long) # 1x1 matrix of integer type, initialized to val = 0\n\nres = decode(m.generate(idx, max_new_tokens=100)[0].tolist())\n\nprint(''.join(res))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:33.967412Z","iopub.execute_input":"2025-04-03T05:30:33.967736Z","iopub.status.idle":"2025-04-03T05:30:34.033858Z","shell.execute_reply.started":"2025-04-03T05:30:33.967709Z","shell.execute_reply":"2025-04-03T05:30:34.032520Z"}},"outputs":[{"name":"stdout","text":"\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# First Generation & Training\n- This looks like garbage because we use a very simple bigram model where the next token predicted depends only on the current token, not the history\n- Next we will try and train it and see if it improves!","metadata":{}},{"cell_type":"code","source":"# lets try the same model on more data\nbatch_size = 32\n\n# pick an optimizer, SGD or ADAM\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nfor steps in range(10000):\n    # get sample batch data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True) # zero out grad from prev steps\n    loss.backward() # get gradients for all pramaters\n    optimizer.step() # update based on gradients\n\nprint(loss.item()) # look at loss improvement","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:34.035006Z","iopub.execute_input":"2025-04-03T05:30:34.035360Z","iopub.status.idle":"2025-04-03T05:30:57.151469Z","shell.execute_reply.started":"2025-04-03T05:30:34.035334Z","shell.execute_reply":"2025-04-03T05:30:57.150196Z"}},"outputs":[{"name":"stdout","text":"2.5727508068084717\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# lets look at model generation after 10k steps of learning\n\nidx = torch.zeros((1,1), dtype=torch.long) # 1x1 matrix of integer type, initialized to val = 0\n\nres = decode(m.generate(idx, max_new_tokens=100)[0].tolist())\n\nprint(''.join(res))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:57.152647Z","iopub.execute_input":"2025-04-03T05:30:57.153161Z","iopub.status.idle":"2025-04-03T05:30:57.182770Z","shell.execute_reply.started":"2025-04-03T05:30:57.153130Z","shell.execute_reply":"2025-04-03T05:30:57.181637Z"}},"outputs":[{"name":"stdout","text":"\nIyoteng h hasbe pave pirance\nRie hicomyonthar's\nPlinseard ith henoure wounonthioneir thondy, y helti\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# We have Shakespear! (From Temu)\n- This looks MUCH better after 10k steps of training!\n- Still not english words, but it has the style and formating of a Shakespear play","metadata":{}},{"cell_type":"code","source":"B,T,C = 4,8,2\n\nx = torch.randn(B,T,C)\nx.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:57.183948Z","iopub.execute_input":"2025-04-03T05:30:57.184345Z","iopub.status.idle":"2025-04-03T05:30:57.192090Z","shell.execute_reply.started":"2025-04-03T05:30:57.184315Z","shell.execute_reply":"2025-04-03T05:30:57.190909Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 2])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:57.193294Z","iopub.execute_input":"2025-04-03T05:30:57.193596Z","iopub.status.idle":"2025-04-03T05:30:57.213303Z","shell.execute_reply.started":"2025-04-03T05:30:57.193556Z","shell.execute_reply":"2025-04-03T05:30:57.211899Z"}},"outputs":[{"name":"stdout","text":"tensor([[[-3.5685e-01, -1.4841e+00],\n         [ 1.4718e-03, -9.1119e-01],\n         [-4.2776e-01, -1.4516e+00],\n         [ 5.2601e-01, -2.5594e-02],\n         [ 4.3046e-01, -9.9963e-01],\n         [-6.9346e-01,  9.3705e-01],\n         [-1.1372e+00, -1.1862e+00],\n         [-9.0355e-01, -1.1036e+00]],\n\n        [[-1.7401e-01, -1.6100e-01],\n         [-1.1436e+00,  1.3350e+00],\n         [ 8.9359e-01, -7.1139e-01],\n         [ 5.7432e-01, -2.3607e+00],\n         [-1.3678e+00,  2.9997e-01],\n         [ 5.2963e-02, -1.6832e+00],\n         [-1.5120e+00, -1.3893e+00],\n         [ 5.5746e-03, -5.0205e-02]],\n\n        [[ 4.9166e-01, -6.4735e-01],\n         [-8.6032e-01, -3.3420e-01],\n         [ 7.4586e-01, -5.0364e-01],\n         [ 1.1548e+00, -9.7437e-02],\n         [-3.2985e-01, -2.0465e+00],\n         [-1.8303e+00,  7.6260e-01],\n         [-5.6973e-01,  3.4038e-01],\n         [-1.2382e+00,  2.3273e+00]],\n\n        [[ 3.3267e-01, -9.5242e-02],\n         [-1.3305e-01,  7.8029e-01],\n         [-6.7658e-01,  6.5710e-01],\n         [-6.7610e-01, -1.3184e+00],\n         [-2.3822e-01, -7.9299e-01],\n         [-3.0160e+00, -6.3886e-02],\n         [ 7.6752e-01,  3.3313e-01],\n         [-7.0711e-01, -2.5034e-01]]])\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tx = x[0, :, :] # slice first block\n\nprint(tx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:30:57.218335Z","iopub.execute_input":"2025-04-03T05:30:57.218670Z","iopub.status.idle":"2025-04-03T05:30:57.231775Z","shell.execute_reply.started":"2025-04-03T05:30:57.218646Z","shell.execute_reply":"2025-04-03T05:30:57.230579Z"}},"outputs":[{"name":"stdout","text":"tensor([[-3.5685e-01, -1.4841e+00],\n        [ 1.4718e-03, -9.1119e-01],\n        [-4.2776e-01, -1.4516e+00],\n        [ 5.2601e-01, -2.5594e-02],\n        [ 4.3046e-01, -9.9963e-01],\n        [-6.9346e-01,  9.3705e-01],\n        [-1.1372e+00, -1.1862e+00],\n        [-9.0355e-01, -1.1036e+00]])\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Self-Attention\n\n- Self-attention block of 1 head is shown below\n- This is improtant as it allows all nodes (or tokens) to have information about all other nodes in the group\n- Other attentions can be used for certain cases such as cross-attention where the nodes take input from another group","metadata":{}},{"cell_type":"code","source":"# self-attention block\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# a single Head of self attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\n# every node will be associated with a key, query, and value vector\nk = key(x) # (B, T, 16)\nq = query(x)\nv = value(x)\n\n# normalize by channels\nwei = q @ k.transpose(-2, -1)*C**(-0.5) # matrix multiplication of (B, T, 16) x (B, 16, T) = (B, T, T)\n\n# we will use a triangular matrix to aggregate the wighted sums for each token\ntril = torch.tril(torch.ones(T,T))\nwei = wei.masked_fill(tril==0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\n# (B, T, T) x (B, T, 16)\nout = wei @ v # we will use the wieghts from the affinity of k*q for each and multiply by the nodes current value\n\nout.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:44:19.260337Z","iopub.execute_input":"2025-04-03T05:44:19.260674Z","iopub.status.idle":"2025-04-03T05:44:19.275808Z","shell.execute_reply.started":"2025-04-03T05:44:19.260649Z","shell.execute_reply":"2025-04-03T05:44:19.274567Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 16])"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"out[0] # started with B = 4 batches of T = block size 8 (4, 8) --> (4, 8, 16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:44:28.792405Z","iopub.execute_input":"2025-04-03T05:44:28.792810Z","iopub.status.idle":"2025-04-03T05:44:28.802607Z","shell.execute_reply.started":"2025-04-03T05:44:28.792782Z","shell.execute_reply":"2025-04-03T05:44:28.801435Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}