{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\n\n- Building a GPT from scratch, following Andrej Karpathy's lesson at: https://youtu.be/kCc8FmEb1nY?si=4AZNnwmnb4XflQT8\n- Character level model, trained on Shakespears works","metadata":{}},{"cell_type":"code","source":"# clone repo for input text files\n!git clone https://github.com/karpathy/ng-video-lecture.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:02.137204Z","iopub.execute_input":"2025-03-25T05:25:02.137582Z","iopub.status.idle":"2025-03-25T05:25:03.077882Z","shell.execute_reply.started":"2025-03-25T05:25:02.137554Z","shell.execute_reply":"2025-03-25T05:25:03.076546Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'ng-video-lecture'...\nremote: Enumerating objects: 64, done.\u001b[K\nremote: Counting objects: 100% (32/32), done.\u001b[K\nremote: Compressing objects: 100% (14/14), done.\u001b[K\nremote: Total 64 (delta 22), reused 18 (delta 18), pack-reused 32 (from 1)\u001b[K\nReceiving objects: 100% (64/64), 441.23 KiB | 11.92 MiB/s, done.\nResolving deltas: 100% (23/23), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# open file we will use to train on\nfile_path = '/kaggle/working/ng-video-lecture/input.txt'\nwith open(file_path, 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:11.089229Z","iopub.execute_input":"2025-03-25T05:25:11.089691Z","iopub.status.idle":"2025-03-25T05:25:11.097843Z","shell.execute_reply.started":"2025-03-25T05:25:11.089653Z","shell.execute_reply":"2025-03-25T05:25:11.096436Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(\"Total chracters in data set:\", len(text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:11.100435Z","iopub.execute_input":"2025-03-25T05:25:11.100766Z","iopub.status.idle":"2025-03-25T05:25:11.118760Z","shell.execute_reply.started":"2025-03-25T05:25:11.100739Z","shell.execute_reply":"2025-03-25T05:25:11.117558Z"}},"outputs":[{"name":"stdout","text":"Total chracters in data set: 1115394\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# some samples from the training set\nprint(text[:1000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:11.120501Z","iopub.execute_input":"2025-03-25T05:25:11.121023Z","iopub.status.idle":"2025-03-25T05:25:11.139855Z","shell.execute_reply.started":"2025-03-25T05:25:11.120989Z","shell.execute_reply":"2025-03-25T05:25:11.138508Z"}},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# all unique characters\nchars = set(text)\nvocab_size = len(chars)\nprint(f\"{vocab_size} char set:\", ''.join(sorted(list(chars))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:11.142406Z","iopub.execute_input":"2025-03-25T05:25:11.142806Z","iopub.status.idle":"2025-03-25T05:25:11.178937Z","shell.execute_reply.started":"2025-03-25T05:25:11.142775Z","shell.execute_reply":"2025-03-25T05:25:11.177577Z"}},"outputs":[{"name":"stdout","text":"65 char set: \n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"sorted_chars = sorted(list(chars))\nfor i in range(vocab_size):\n    # checking space & new line characters\n    c = sorted_chars[i]\n    print(f\"{i}: {ord(c)} '{c}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:11.180773Z","iopub.execute_input":"2025-03-25T05:25:11.181214Z","iopub.status.idle":"2025-03-25T05:25:11.215832Z","shell.execute_reply.started":"2025-03-25T05:25:11.181181Z","shell.execute_reply":"2025-03-25T05:25:11.214518Z"}},"outputs":[{"name":"stdout","text":"0: 10 '\n'\n1: 32 ' '\n2: 33 '!'\n3: 36 '$'\n4: 38 '&'\n5: 39 '''\n6: 44 ','\n7: 45 '-'\n8: 46 '.'\n9: 51 '3'\n10: 58 ':'\n11: 59 ';'\n12: 63 '?'\n13: 65 'A'\n14: 66 'B'\n15: 67 'C'\n16: 68 'D'\n17: 69 'E'\n18: 70 'F'\n19: 71 'G'\n20: 72 'H'\n21: 73 'I'\n22: 74 'J'\n23: 75 'K'\n24: 76 'L'\n25: 77 'M'\n26: 78 'N'\n27: 79 'O'\n28: 80 'P'\n29: 81 'Q'\n30: 82 'R'\n31: 83 'S'\n32: 84 'T'\n33: 85 'U'\n34: 86 'V'\n35: 87 'W'\n36: 88 'X'\n37: 89 'Y'\n38: 90 'Z'\n39: 97 'a'\n40: 98 'b'\n41: 99 'c'\n42: 100 'd'\n43: 101 'e'\n44: 102 'f'\n45: 103 'g'\n46: 104 'h'\n47: 105 'i'\n48: 106 'j'\n49: 107 'k'\n50: 108 'l'\n51: 109 'm'\n52: 110 'n'\n53: 111 'o'\n54: 112 'p'\n55: 113 'q'\n56: 114 'r'\n57: 115 's'\n58: 116 't'\n59: 117 'u'\n60: 118 'v'\n61: 119 'w'\n62: 120 'x'\n63: 121 'y'\n64: 122 'z'\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Encoder/Decoder: each character maps to an integer which its the indx in the sorted array\nc_to_i = {c:i for i,c in enumerate(sorted_chars)}\ni_to_c = {i:c for i,c in enumerate(sorted_chars)}\n\ndef encode(s):\n    # takes input string to encode\n    return [c_to_i[c] for c in s]\n\n\ndef decode(l):\n    # takes an array of integers to decode\n    return [i_to_c[i] for i in l]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:11.217045Z","iopub.execute_input":"2025-03-25T05:25:11.217419Z","iopub.status.idle":"2025-03-25T05:25:11.224827Z","shell.execute_reply.started":"2025-03-25T05:25:11.217381Z","shell.execute_reply":"2025-03-25T05:25:11.223764Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(encode(\"Hello World this is GPT\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:11.226246Z","iopub.execute_input":"2025-03-25T05:25:11.226626Z","iopub.status.idle":"2025-03-25T05:25:11.247485Z","shell.execute_reply.started":"2025-03-25T05:25:11.226583Z","shell.execute_reply":"2025-03-25T05:25:11.246220Z"}},"outputs":[{"name":"stdout","text":"[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 1, 58, 46, 47, 57, 1, 47, 57, 1, 19, 28, 32]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"x = encode(\"Hello World this is GPT\")\nprint(decode(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:11.248687Z","iopub.execute_input":"2025-03-25T05:25:11.249101Z","iopub.status.idle":"2025-03-25T05:25:11.270100Z","shell.execute_reply.started":"2025-03-25T05:25:11.249062Z","shell.execute_reply":"2025-03-25T05:25:11.268801Z"}},"outputs":[{"name":"stdout","text":"['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', 'l', 'd', ' ', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 'G', 'P', 'T']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"- Many other encoding schemas are possible, chat GPT and others mainly use **sub-word** tokenizers\n- Here this simple model has a tradeoff that a small vocab size for the encoder == large vector encodings\n- if we had a much larger vocab size == smaller vector encoding","metadata":{}},{"cell_type":"markdown","source":"# Pytorch\n- We will use pytorch here to generate the data as a tensor for training","metadata":{}},{"cell_type":"code","source":"import torch\ndata = torch.tensor(encode(text)) # Tensor (m,) == 1D vector --> 1D array\nprint(data.shape, data.dtype)\nprint(data[:1000]) # 1000 characters encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:11.273308Z","iopub.execute_input":"2025-03-25T05:25:11.273609Z","iopub.status.idle":"2025-03-25T05:25:15.590627Z","shell.execute_reply.started":"2025-03-25T05:25:11.273585Z","shell.execute_reply":"2025-03-25T05:25:15.589580Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# training and validation splits\nn = int(0.9*len(data))\ntrain = data[:n]\nval = data[n:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:15.591981Z","iopub.execute_input":"2025-03-25T05:25:15.592440Z","iopub.status.idle":"2025-03-25T05:25:15.597118Z","shell.execute_reply.started":"2025-03-25T05:25:15.592412Z","shell.execute_reply":"2025-03-25T05:25:15.595989Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# we will train on chunks of the data\nblock_size = 8 # this will be the max context X to predict target Y\ntrain[:block_size + 1] # we can train on 8 subarrays/instances and predict the next 1 given 9 total seq len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:15.598303Z","iopub.execute_input":"2025-03-25T05:25:15.598701Z","iopub.status.idle":"2025-03-25T05:25:15.621373Z","shell.execute_reply.started":"2025-03-25T05:25:15.598666Z","shell.execute_reply":"2025-03-25T05:25:15.620485Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# example of a training instance\nx = train[:block_size]\ny = train[1:block_size + 1]\n\nfor i in range(block_size):\n    context = x[:i+1] # given the context predict y\n    target = y[i] # only predicts the next chracter in a given seq\n    print(\"Context:\", context, \"Target:\", target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:15.622341Z","iopub.execute_input":"2025-03-25T05:25:15.622644Z","iopub.status.idle":"2025-03-25T05:25:15.653114Z","shell.execute_reply.started":"2025-03-25T05:25:15.622620Z","shell.execute_reply":"2025-03-25T05:25:15.652029Z"}},"outputs":[{"name":"stdout","text":"Context: tensor([18]) Target: tensor(47)\nContext: tensor([18, 47]) Target: tensor(56)\nContext: tensor([18, 47, 56]) Target: tensor(57)\nContext: tensor([18, 47, 56, 57]) Target: tensor(58)\nContext: tensor([18, 47, 56, 57, 58]) Target: tensor(1)\nContext: tensor([18, 47, 56, 57, 58,  1]) Target: tensor(15)\nContext: tensor([18, 47, 56, 57, 58,  1, 15]) Target: tensor(47)\nContext: tensor([18, 47, 56, 57, 58,  1, 15, 47]) Target: tensor(58)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# create batch processing\ntorch.manual_seed(1337)\nbatch_size = 4\nblock_size = 8\n\ndef get_batch(split):\n    data = train if split == 'train' else val\n    ix = torch.randint(len(data) - block_size, (batch_size,)) # outputs 1D vector of len == batch_size\n    x = torch.stack([data[i:i+block_size] for i in ix]) # stacks the 1D vectors --> (4,8) 4 rows x 8 cols\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # (4,8) we will make 8 predictions y on each X\n    return x, y\n\nxb, yb = get_batch('train')\nprint(f\"Inputs: {xb.shape} \\n{xb} \\nTargets: {yb.shape} \\n{yb}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:15.653978Z","iopub.execute_input":"2025-03-25T05:25:15.654262Z","iopub.status.idle":"2025-03-25T05:25:15.687732Z","shell.execute_reply.started":"2025-03-25T05:25:15.654235Z","shell.execute_reply":"2025-03-25T05:25:15.686623Z"}},"outputs":[{"name":"stdout","text":"Inputs: torch.Size([4, 8]) \ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]]) \nTargets: torch.Size([4, 8]) \ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Total training samples per batch == batch_size * block_size == 8*4 == 32\nfor b in range(batch_size):\n    for t in range(block_size):\n        context = xb[b, :t+1] # sliding window context\n        target = yb[b,t] # predict the next chracter c\n        print(f\"Context: {context} Target: {target}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:25:15.688737Z","iopub.execute_input":"2025-03-25T05:25:15.689038Z","iopub.status.idle":"2025-03-25T05:25:15.711371Z","shell.execute_reply.started":"2025-03-25T05:25:15.689011Z","shell.execute_reply":"2025-03-25T05:25:15.710476Z"}},"outputs":[{"name":"stdout","text":"Context: tensor([24]) Target: 43\nContext: tensor([24, 43]) Target: 58\nContext: tensor([24, 43, 58]) Target: 5\nContext: tensor([24, 43, 58,  5]) Target: 57\nContext: tensor([24, 43, 58,  5, 57]) Target: 1\nContext: tensor([24, 43, 58,  5, 57,  1]) Target: 46\nContext: tensor([24, 43, 58,  5, 57,  1, 46]) Target: 43\nContext: tensor([24, 43, 58,  5, 57,  1, 46, 43]) Target: 39\nContext: tensor([44]) Target: 53\nContext: tensor([44, 53]) Target: 56\nContext: tensor([44, 53, 56]) Target: 1\nContext: tensor([44, 53, 56,  1]) Target: 58\nContext: tensor([44, 53, 56,  1, 58]) Target: 46\nContext: tensor([44, 53, 56,  1, 58, 46]) Target: 39\nContext: tensor([44, 53, 56,  1, 58, 46, 39]) Target: 58\nContext: tensor([44, 53, 56,  1, 58, 46, 39, 58]) Target: 1\nContext: tensor([52]) Target: 58\nContext: tensor([52, 58]) Target: 1\nContext: tensor([52, 58,  1]) Target: 58\nContext: tensor([52, 58,  1, 58]) Target: 46\nContext: tensor([52, 58,  1, 58, 46]) Target: 39\nContext: tensor([52, 58,  1, 58, 46, 39]) Target: 58\nContext: tensor([52, 58,  1, 58, 46, 39, 58]) Target: 1\nContext: tensor([52, 58,  1, 58, 46, 39, 58,  1]) Target: 46\nContext: tensor([25]) Target: 17\nContext: tensor([25, 17]) Target: 27\nContext: tensor([25, 17, 27]) Target: 10\nContext: tensor([25, 17, 27, 10]) Target: 0\nContext: tensor([25, 17, 27, 10,  0]) Target: 21\nContext: tensor([25, 17, 27, 10,  0, 21]) Target: 1\nContext: tensor([25, 17, 27, 10,  0, 21,  1]) Target: 54\nContext: tensor([25, 17, 27, 10,  0, 21,  1, 54]) Target: 39\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Bigram Language Model\n- Simplest possible NN model for LLM","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token reads off the logits for the next token from this lookup table of 65x65\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n    \n    def forward(self, idx, targets):\n        logits = self.token_embedding_table(idx) # (B,T,Channels=vocab_size) == (4,8,65)\n\n        # reshape input tensor to match pytorch docs F.cross_entropy\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C) # 2D tensor\n        targets = targets.view(B*T) # 1D Tensor\n\n        # calculate the loss\n        loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb) # __call__ is enabled for the \"forward\" method of classes\n\nprint(logits.shape)\nprint(loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:52:59.115864Z","iopub.execute_input":"2025-03-25T05:52:59.116438Z","iopub.status.idle":"2025-03-25T05:52:59.176138Z","shell.execute_reply.started":"2025-03-25T05:52:59.116395Z","shell.execute_reply":"2025-03-25T05:52:59.174786Z"}},"outputs":[{"name":"stdout","text":"torch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Now lets see how this compares to the theoretical loss which is realted to entropy == number of states\nimport numpy as np\n\nprint(-np.log(1/65))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:54:48.165472Z","iopub.execute_input":"2025-03-25T05:54:48.165831Z","iopub.status.idle":"2025-03-25T05:54:48.172547Z","shell.execute_reply.started":"2025-03-25T05:54:48.165806Z","shell.execute_reply":"2025-03-25T05:54:48.171392Z"}},"outputs":[{"name":"stdout","text":"4.174387269895637\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# initial predictions are not that great due to thier entropy\n# Here we will look at model generation\n\n# min 37 (-10 mins or so)\n\n# continuing here","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}